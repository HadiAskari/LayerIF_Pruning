sparsegpt_ww:
{'results': {'boolq': {'acc': 0.7345565749235474, 'acc_stderr': 0.007723090983590468}, 'arc_easy': {'acc': 0.688973063973064, 'acc_stderr': 0.009498790639757615, 'acc_norm': 0.6548821548821548, 'acc_norm_stderr': 0.009755139387152025}, 'hellaswag': {'acc': 0.4792869946225851, 'acc_stderr': 0.004985498055190364, 'acc_norm': 0.6538538139812786, 'acc_norm_stderr': 0.004747682003491436}, 'openbookqa': {'acc': 0.23, 'acc_stderr': 0.018839050391123137, 'acc_norm': 0.368, 'acc_norm_stderr': 0.021588982568353544}, 'rte': {'acc': 0.555956678700361, 'acc_stderr': 0.029907396333795987}, 'winogrande': {'acc': 0.681136543014996, 'acc_stderr': 0.01309792842008877}, 'arc_challenge': {'acc': 0.35238907849829354, 'acc_stderr': 0.013960142600598682, 'acc_norm': 0.3771331058020478, 'acc_norm_stderr': 0.014163366896192582}}, 'versions': {'boolq': 1, 'arc_easy': 0, 'hellaswag': 0, 'openbookqa': 0, 'rte': 0, 'winogrande': 0, 'arc_challenge': 0}, 'config': {'model': 'hf-causal-experimental', 'model_args': 'pretrained=mistralai/Mistral-7B-v0.1,cache_dir=./llm_weights,use_accelerate=True', 'num_fewshot': 0, 'batch_size': None, 'batch_sizes': [], 'device': None, 'no_cache': True, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': {}}}
