wanda_ww:
{'results': {'hellaswag': {'acc': 0.5417247560246963, 'acc_stderr': 0.004972377085916329, 'acc_norm': 0.7310296753634734, 'acc_norm_stderr': 0.004425182676353218}, 'rte': {'acc': 0.5848375451263538, 'acc_stderr': 0.02966006629089349}, 'arc_easy': {'acc': 0.7525252525252525, 'acc_stderr': 0.008855114414834709, 'acc_norm': 0.7466329966329966, 'acc_norm_stderr': 0.008924765424529269}, 'boolq': {'acc': 0.8262996941896025, 'acc_stderr': 0.006626155556927151}, 'arc_challenge': {'acc': 0.4197952218430034, 'acc_stderr': 0.014422181226303028, 'acc_norm': 0.44283276450511944, 'acc_norm_stderr': 0.014515573873348902}, 'winogrande': {'acc': 0.7024467245461721, 'acc_stderr': 0.012849085254614647}, 'openbookqa': {'acc': 0.258, 'acc_stderr': 0.019586711785215837, 'acc_norm': 0.396, 'acc_norm_stderr': 0.021893529941665813}}, 'versions': {'hellaswag': 0, 'rte': 0, 'arc_easy': 0, 'boolq': 1, 'arc_challenge': 0, 'winogrande': 0, 'openbookqa': 0}, 'config': {'model': 'hf-causal-experimental', 'model_args': 'pretrained=mistralai/Mistral-7B-v0.1,cache_dir=./llm_weights,use_accelerate=True', 'num_fewshot': 0, 'batch_size': None, 'batch_sizes': [], 'device': None, 'no_cache': True, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': {}}}
