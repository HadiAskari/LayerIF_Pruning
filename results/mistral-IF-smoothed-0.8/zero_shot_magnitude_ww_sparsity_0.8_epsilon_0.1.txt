magnitude_ww:
{'results': {'rte': {'acc': 0.51985559566787, 'acc_stderr': 0.030072723167317184}, 'winogrande': {'acc': 0.48145224940805054, 'acc_stderr': 0.014042813708888378}, 'hellaswag': {'acc': 0.2568213503286198, 'acc_stderr': 0.00435987151963954, 'acc_norm': 0.26379207329217286, 'acc_norm_stderr': 0.004397872471854923}, 'boolq': {'acc': 0.4694189602446483, 'acc_stderr': 0.008728682900189723}, 'openbookqa': {'acc': 0.142, 'acc_stderr': 0.0156256302478103, 'acc_norm': 0.25, 'acc_norm_stderr': 0.019384310743640384}, 'arc_challenge': {'acc': 0.2167235494880546, 'acc_stderr': 0.012040156713481192, 'acc_norm': 0.27047781569965873, 'acc_norm_stderr': 0.012980954547659554}, 'arc_easy': {'acc': 0.2558922558922559, 'acc_stderr': 0.008953950243013986, 'acc_norm': 0.27146464646464646, 'acc_norm_stderr': 0.009125362970360623}}, 'versions': {'rte': 0, 'winogrande': 0, 'hellaswag': 0, 'boolq': 1, 'openbookqa': 0, 'arc_challenge': 0, 'arc_easy': 0}, 'config': {'model': 'hf-causal-experimental', 'model_args': 'pretrained=mistralai/Mistral-7B-v0.1,cache_dir=./llm_weights,use_accelerate=True', 'num_fewshot': 0, 'batch_size': None, 'batch_sizes': [], 'device': None, 'no_cache': True, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': {}}}
