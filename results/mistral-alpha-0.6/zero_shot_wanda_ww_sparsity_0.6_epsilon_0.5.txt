wanda_ww:
{'results': {'boolq': {'acc': 0.6311926605504588, 'acc_stderr': 0.00843865607975907}, 'winogrande': {'acc': 0.6550907655880032, 'acc_stderr': 0.013359379805033692}, 'openbookqa': {'acc': 0.192, 'acc_stderr': 0.017632180454360987, 'acc_norm': 0.314, 'acc_norm_stderr': 0.020776701920308997}, 'hellaswag': {'acc': 0.41963752240589525, 'acc_stderr': 0.004924910433106358, 'acc_norm': 0.5790679147580163, 'acc_norm_stderr': 0.004926996830194251}, 'arc_easy': {'acc': 0.6022727272727273, 'acc_stderr': 0.010042861602178058, 'acc_norm': 0.5509259259259259, 'acc_norm_stderr': 0.010206428316323365}, 'arc_challenge': {'acc': 0.2713310580204778, 'acc_stderr': 0.012993807727545797, 'acc_norm': 0.29692832764505117, 'acc_norm_stderr': 0.013352025976725222}, 'rte': {'acc': 0.5631768953068592, 'acc_stderr': 0.029855247390314945}}, 'versions': {'boolq': 1, 'winogrande': 0, 'openbookqa': 0, 'hellaswag': 0, 'arc_easy': 0, 'arc_challenge': 0, 'rte': 0}, 'config': {'model': 'hf-causal-experimental', 'model_args': 'pretrained=mistralai/Mistral-7B-v0.1,cache_dir=./llm_weights,use_accelerate=True', 'num_fewshot': 0, 'batch_size': None, 'batch_sizes': [], 'device': None, 'no_cache': True, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': {}}}
