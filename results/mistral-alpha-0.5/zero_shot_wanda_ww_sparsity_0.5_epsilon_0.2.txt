wanda_ww:
{'results': {'arc_easy': {'acc': 0.7495791245791246, 'acc_stderr': 0.008890213675113954, 'acc_norm': 0.7394781144781145, 'acc_norm_stderr': 0.009006435890336588}, 'arc_challenge': {'acc': 0.4087030716723549, 'acc_stderr': 0.014365750345427006, 'acc_norm': 0.4351535836177474, 'acc_norm_stderr': 0.014487986197186052}, 'hellaswag': {'acc': 0.5416251742680741, 'acc_stderr': 0.004972460206842306, 'acc_norm': 0.7362079267078271, 'acc_norm_stderr': 0.004397872471854932}, 'winogrande': {'acc': 0.691397000789266, 'acc_stderr': 0.01298216020092658}, 'rte': {'acc': 0.5848375451263538, 'acc_stderr': 0.02966006629089349}, 'openbookqa': {'acc': 0.254, 'acc_stderr': 0.019486596801643385, 'acc_norm': 0.404, 'acc_norm_stderr': 0.021966635293832915}, 'boolq': {'acc': 0.8097859327217125, 'acc_stderr': 0.006864342895848143}}, 'versions': {'arc_easy': 0, 'arc_challenge': 0, 'hellaswag': 0, 'winogrande': 0, 'rte': 0, 'openbookqa': 0, 'boolq': 1}, 'config': {'model': 'hf-causal-experimental', 'model_args': 'pretrained=mistralai/Mistral-7B-v0.1,cache_dir=./llm_weights,use_accelerate=True', 'num_fewshot': 0, 'batch_size': None, 'batch_sizes': [], 'device': None, 'no_cache': True, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': {}}}
