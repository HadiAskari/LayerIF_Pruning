sparsegpt_ww:
{'results': {'boolq': {'acc': 0.8308868501529052, 'acc_stderr': 0.006556199674684518}, 'arc_easy': {'acc': 0.7643097643097643, 'acc_stderr': 0.008709108323214464, 'acc_norm': 0.7617845117845118, 'acc_norm_stderr': 0.008741163824469185}, 'hellaswag': {'acc': 0.5584544911372237, 'acc_stderr': 0.004955564650016176, 'acc_norm': 0.7550288787094205, 'acc_norm_stderr': 0.004291911350430708}, 'openbookqa': {'acc': 0.264, 'acc_stderr': 0.0197328855859221, 'acc_norm': 0.398, 'acc_norm_stderr': 0.021912377885779967}, 'winogrande': {'acc': 0.7119179163378059, 'acc_stderr': 0.012727884724248115}, 'arc_challenge': {'acc': 0.4325938566552901, 'acc_stderr': 0.014478005694182528, 'acc_norm': 0.45819112627986347, 'acc_norm_stderr': 0.014560220308714702}, 'rte': {'acc': 0.6425992779783394, 'acc_stderr': 0.028846510722612007}}, 'versions': {'boolq': 1, 'arc_easy': 0, 'hellaswag': 0, 'openbookqa': 0, 'winogrande': 0, 'arc_challenge': 0, 'rte': 0}, 'config': {'model': 'hf-causal-experimental', 'model_args': 'pretrained=mistralai/Mistral-7B-v0.1,cache_dir=./llm_weights,use_accelerate=True', 'num_fewshot': 0, 'batch_size': None, 'batch_sizes': [], 'device': None, 'no_cache': True, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': {}}}
